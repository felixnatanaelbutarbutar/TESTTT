```mermaid
flowchart TD
    %% ========== DATA PREPARATION ==========
    A[ðŸ“Š Dataset CEFR<br/>Words/Sentences with Labels A1-C2] --> B[ðŸ”§ Data Preprocessing]
    
    B --> B1[Text Normalization<br/>â€¢ Lowercase conversion<br/>â€¢ Remove special chars<br/>â€¢ Handle punctuation]
    B1 --> B2[BERT Tokenization<br/>â€¢ WordPiece tokenization<br/>â€¢ Add [CLS] and [SEP] tokens<br/>â€¢ Padding/truncation]
    B2 --> B3[Optional: Linguistic Processing<br/>â€¢ Lemmatization<br/>â€¢ POS tagging<br/>â€¢ Syntax features]
    
    %% ========== DATA SPLIT ==========
    B3 --> DS[ðŸ“ˆ Data Split<br/>Train: 70% | Val: 15% | Test: 15%]
    
    %% ========== MODEL ARCHITECTURE ==========
    DS --> C[ðŸ¤– BERT Base Model<br/>Pre-trained Encoder<br/>12 layers, 768 hidden units]
    C --> D[ðŸŽ¯ Pooling Strategy]
    D --> D1[CLS Token Pooling]
    D --> D2[Mean Pooling]
    D --> D3[Max Pooling]
    
    D1 --> E[ðŸ§  Classification Head]
    D2 --> E
    D3 --> E
    
    E --> E1[Dense Layer<br/>768 â†’ 256 units<br/>ReLU activation]
    E1 --> E2[Dropout Layer<br/>p = 0.3]
    E2 --> E3[Output Layer<br/>256 â†’ 6 units<br/>Softmax activation]
    
    %% ========== TRAINING PROCESS ==========
    E3 --> F[ðŸŽ¯ Training Loop]
    F --> F1[Loss Function<br/>Categorical Cross-Entropy<br/>+ Label Smoothing]
    F --> F2[Optimizer<br/>AdamW<br/>lr=2e-5, weight_decay=0.01]
    F --> F3[Learning Rate Scheduler<br/>Linear warmup (10% steps)<br/>+ Cosine decay]
    F --> F4[Regularization<br/>â€¢ Gradient clipping<br/>â€¢ Early stopping<br/>â€¢ L2 regularization]
    
    %% ========== VALIDATION ==========
    F --> V[ðŸ“Š Validation & Metrics]
    V --> V1[Primary Metrics<br/>â€¢ Accuracy<br/>â€¢ Macro F1-score<br/>â€¢ Weighted F1-score]
    V --> V2[Additional Analysis<br/>â€¢ Confusion matrix<br/>â€¢ Per-class precision/recall<br/>â€¢ Classification report]
    V --> V3[Model Selection<br/>Best model on validation F1]
    
    %% ========== INFERENCE PIPELINE ==========
    K[ðŸ†• New Input<br/>Word/Sentence] --> L[ðŸ”§ Preprocessing Pipeline]
    L --> L1[Text Normalization<br/>Same as training]
    L1 --> L2[BERT Tokenization<br/>Same tokenizer as training]
    L2 --> C
    
    %% ========== OUTPUT ==========
    E3 --> M[ðŸ“‹ Prediction Output]
    M --> M1[CEFR Level Prediction<br/>A1, A2, B1, B2, C1, C2]
    M --> M2[Confidence Scores<br/>Probability distribution<br/>across all levels]
    M --> M3[Additional Info<br/>â€¢ Top-2 predictions<br/>â€¢ Confidence threshold<br/>â€¢ Uncertainty measure]
    
    %% ========== MODEL EVALUATION ==========
    V3 --> TEST[ðŸ§ª Final Test Evaluation]
    TEST --> R[ðŸ“ˆ Results & Analysis<br/>â€¢ Test accuracy<br/>â€¢ Error analysis<br/>â€¢ Linguistic insights]
    
    %% Styling
    classDef dataClass fill:#e1f5fe,stroke:#01579b,stroke-width:2px
    classDef modelClass fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    classDef trainingClass fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px
    classDef inferenceClass fill:#fff3e0,stroke:#e65100,stroke-width:2px
    classDef outputClass fill:#fce4ec,stroke:#880e4f,stroke-width:2px
    
    class A,B,B1,B2,B3,DS dataClass
    class C,D,D1,D2,D3,E,E1,E2,E3 modelClass
    class F,F1,F2,F3,F4,V,V1,V2,V3 trainingClass
    class K,L,L1,L2,TEST inferenceClass
    class M,M1,M2,M3,R outputClass
